{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86182d11-e222-4dc7-8af5-3ffa3a927e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from importlib import reload  \n",
    "import numpy as np\n",
    "import torch,time\n",
    "from transformers import BartModel,BartConfig,BartForConditionalGeneration,BartForCausalLM\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8b195ea-9934-4ecf-b54b-e096c439e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "  \n",
    "# setting path to enable import from the parent directory\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3647212-f6c0-4bb4-a44e-68f2f83e0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, score_at_min1=0,patience=100, verbose=False, delta=0, path='checkpoint.pt',\n",
    "                 trace_func=print,save_epochwise=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = score_at_min1\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        self.state_dict_list=[None]*patience\n",
    "        self.improved=0\n",
    "        self.stop_update=0\n",
    "        self.save_model_counter=0\n",
    "        self.save_epochwise=save_epochwise\n",
    "        self.times_improved=0\n",
    "        self.activated=False\n",
    "    def activate(self,s1,s2):\n",
    "        if not self.activated and s1>0 and s2>0: self.activated=True\n",
    "    def __call__(self, score, epoch,model):\n",
    "        if not self.activated: return None\n",
    "        self.save_model_counter = (self.save_model_counter + 1) % 4\n",
    "        if not self.stop_update:\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'\\033[91m The val score  of epoch {epoch} is {score:.4f} \\033[0m')\n",
    "            if score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                self.trace_func(f'\\033[93m EarlyStopping counter: {self.counter} out of {self.patience} \\033[0m')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "                self.improved=0\n",
    "            else:\n",
    "                self.save_checkpoint(score, model,epoch)\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "                self.improved=1\n",
    "        else:\n",
    "            self.improved=0 #not needed though\n",
    "\n",
    "    def save_checkpoint(self, score, model,epoch):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        # if self.verbose:\n",
    "        self.times_improved+=1\n",
    "        self.trace_func(f'\\033[92m Validation score improved ({self.best_score:.4f} --> {score:.4f}). \\033[0m')\n",
    "        if self.save_epochwise:\n",
    "            path=self.path+\"_\"+str(self.times_improved)+\"_\"+str(epoch)\n",
    "        else:\n",
    "            path=self.path\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ceb6ba7-a1dd-4806-ad0b-9d9b0c3d8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import make_dataset\n",
    "import pathlib\n",
    "train=make_dataset(pathlib.Path(\"../data/protechn_corpus_eval/train/\"))\n",
    "val=make_dataset(pathlib.Path(\"../data/protechn_corpus_eval/dev/\"))\n",
    "test=make_dataset(pathlib.Path(\"../data/protechn_corpus_eval/test/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d9e6bb0-c5ca-4d79-9e53-33965ff44e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6a8a72f-1a8e-4f20-b8a5-103c46a576b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import make_bert_dataset,make_bert_testset\n",
    "train_=make_bert_testset(train)\n",
    "val_=make_bert_testset(val)\n",
    "test_=make_bert_testset(test)\n",
    "train_sents=[ list(map(lambda x: x[1] if x[0]==0 else \" \"+x[1], enumerate(i))) for d in train_[0] for i in d]\n",
    "val_sents=[ list(map(lambda x: x[1] if x[0]==0 else \" \"+x[1], enumerate(i))) for d in val_[0] for i in d]\n",
    "test_sents=[ list(map(lambda x: x[1] if x[0]==0 else \" \"+x[1], enumerate(i))) for d in test_[0] for i in d]\n",
    "def create_labels(dataset):\n",
    "    temp=[ set(i)-set(\"O\") for d in dataset[1] for i in d]\n",
    "    return [ next(iter(i)) if len(i)>0 else \"O\"  for i in temp]\n",
    "train_ls=create_labels(train_)\n",
    "val_ls=create_labels(val_)\n",
    "test_ls=create_labels(test_)\n",
    "train_y_txt=[ i for d in train_[1] for i in d]\n",
    "val_y_txt=[ i for d in val_[1] for i in d]\n",
    "test_y_txt=[ i for d in test_[1] for i in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cabafab5-aab5-495f-ae3e-85ed83edf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_set={'Appeal_to_Authority',\n",
    " 'Appeal_to_fear-prejudice',\n",
    " 'Bandwagon',\n",
    " 'Black-and-White_Fallacy',\n",
    " 'Causal_Oversimplification',\n",
    " 'Doubt',\n",
    " 'Exaggeration,Minimisation',\n",
    " 'Flag-Waving',\n",
    " 'Loaded_Language',\n",
    " 'Name_Calling,Labeling',\n",
    " 'O',\n",
    " 'Obfuscation,Intentional_Vagueness,Confusion',\n",
    " 'Red_Herring',\n",
    " 'Reductio_ad_hitlerum',\n",
    " 'Repetition',\n",
    " 'Slogans',\n",
    " 'Straw_Men',\n",
    " 'Thought-terminating_Cliches',\n",
    " 'Whataboutism'}\n",
    "train_idx_bylabel={x: [i for i in range(len(train_ls)) if train_ls[i]==x] for x in labels_set} \n",
    "val_idx_bylabel={x: [i for i in range(len(val_ls)) if val_ls[i]==x] for x in labels_set} \n",
    "test_idx_bylabel={x: [i for i in range(len(test_ls)) if test_ls[i]==x] for x in labels_set} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b24cbc76-be6b-4749-a243-29d0f3a16799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x,y,y_txt,it_is_train=1,pos_or_neg=None,fix_seq_len=256,balance=False,\n",
    "                 specific_label=None,for_protos=False):\n",
    "        self.x=[]\n",
    "        self.attn_mask=[]\n",
    "        self.labels_mask=[]\n",
    "        self.y_txt=[]\n",
    "        self.y=[]\n",
    "        self.labels_ids={}\n",
    "        for i in labels_set:\n",
    "            self.labels_ids[i]=len(self.labels_ids)\n",
    "        self.y_fine_int=[]\n",
    "        it_is_train_proxy=it_is_train\n",
    "        for split_sent,y_tags,y_sent in zip(x,y_txt,y):\n",
    "            if specific_label is not None and specific_label!=y_sent: continue\n",
    "            if pos_or_neg==\"pos\" and y_sent==\"O\": continue\n",
    "            elif pos_or_neg==\"neg\" and y_sent!=\"O\": continue                \n",
    "            if y_sent==\"O\":\n",
    "                it_is_train=0\n",
    "            else:\n",
    "                it_is_train=it_is_train_proxy               \n",
    "            tmp=tokenizer(split_sent,is_split_into_words=False)[\"input_ids\"]\n",
    "            tmp_x=[]\n",
    "            tmp_attn=[]\n",
    "            tmp_y=[]\n",
    "            for i,chunk in enumerate(tmp):\n",
    "                if for_protos and y_tags[i]==\"O\":\n",
    "                    continue\n",
    "                tmp_y.extend([y_tags[i]]*len(chunk))\n",
    "                if y_tags[i]!=\"O\":\n",
    "                    mask=1\n",
    "                else:\n",
    "                    if it_is_train:\n",
    "                        mask=0\n",
    "                    else:\n",
    "                        mask=1\n",
    "                tmp_x.extend(chunk[1:-1])\n",
    "                tmp_attn.extend([mask]*(len(chunk)-2))\n",
    "            tmp_x.append(tokenizer.eos_token_id)\n",
    "            tmp_x.insert(0,tokenizer.bos_token_id)\n",
    "            tmp_attn.append(tmp_attn[-1])\n",
    "            tmp_attn.insert(0,tmp_attn[0])\n",
    "            self.x.append(tmp_x)\n",
    "            self.attn_mask.append(tmp_attn)\n",
    "            self.y_txt.append(tmp_y)\n",
    "            self.y.append(1 if y_sent!=\"O\" else 0)\n",
    "            self.y_fine_int.append(self.labels_ids[y_sent])\n",
    "        for tokid_sent in self.x:\n",
    "            tokid_sent.extend([tokenizer.pad_token_id]*(fix_seq_len-len(tokid_sent)))\n",
    "        for mask_vec in self.attn_mask:\n",
    "            mask_vec.extend([0]*(fix_seq_len-len(mask_vec)))\n",
    "        if balance:\n",
    "            num_pos=np.sum(self.y)\n",
    "            assert num_pos<len(self.y_fine_int)//2\n",
    "            \n",
    "            pos_indices=np.random.choice([i for i in range(len(self.y)) if self.y[i]==1],\n",
    "                                         size=len(self.y)-2*num_pos,replace=True)\n",
    "            self.x.extend([self.x[i] for i in pos_indices])\n",
    "            self.y.extend([1 for i in pos_indices])\n",
    "            self.y_fine_int.extend([self.y_fine_int[i] for i in pos_indices])\n",
    "            self.attn_mask.extend([self.attn_mask[i] for i in pos_indices])\n",
    "        self.fix_seq_len=fix_seq_len\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx],self.attn_mask[idx],self.y[idx]\n",
    "    def collate_fn(self,batch):        \n",
    "        return (torch.LongTensor([i[0] for i in batch]),\n",
    "                torch.Tensor([i[1] for i in batch]),\n",
    "                torch.LongTensor([i[2] for i in batch]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5012b7a8-5ac6-42a6-b5c5-1de4f3b15c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=BinaryClassDataset(train_sents,train_ls,train_y_txt,it_is_train=0,balance=True)\n",
    "val_dataset=BinaryClassDataset(val_sents,val_ls,val_y_txt,it_is_train=0)\n",
    "test_dataset=BinaryClassDataset(test_sents,test_ls,test_y_txt,it_is_train=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "517c1969-3d65-4a23-aca2-12d9cb034fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl=torch.utils.data.DataLoader(train_dataset,batch_size=20,shuffle=True,\n",
    "                                     collate_fn=train_dataset.collate_fn)\n",
    "val_dl=torch.utils.data.DataLoader(val_dataset,batch_size=128,shuffle=False,\n",
    "                                     collate_fn=val_dataset.collate_fn)\n",
    "test_dl=torch.utils.data.DataLoader(test_dataset,batch_size=128,shuffle=False,\n",
    "                                     collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8127b463-edb2-4301-8063-f8793d2b5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_logs(file,info,epoch,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1):\n",
    "    logs=[]\n",
    "    s=\" \".join((info+\" epoch\",str(epoch),\"Total loss %.4f\"%(val_loss),\"\\n\"))\n",
    "    logs.append(s)\n",
    "    print(s)\n",
    "    s=\" \".join((info+\" epoch\",str(epoch),\"Prec\",str(mac_val_prec),\"\\n\"))\n",
    "    logs.append(s)\n",
    "    print(s)\n",
    "    s=\" \".join((info+\" epoch\",str(epoch),\"Recall\",str(mac_val_rec),\"\\n\"))\n",
    "    logs.append(s)\n",
    "    print(s)\n",
    "    s=\" \".join((info+\" epoch\",str(epoch),\"F1\",str(mac_val_f1),\"\\n\"))\n",
    "    logs.append(s)\n",
    "    print(s)\n",
    "#     print(\"epoch\",epoch,\"MICRO val precision %.4f, recall %.4f, f1 %.4f,\"%(mic_val_prec,mic_val_rec,mic_val_f1))\n",
    "    print() \n",
    "    logs.append(\"\\n\")\n",
    "    f=open(file,\"a\")\n",
    "    f.writelines(logs)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58e8ba21-15d4-4bed-92ef-a40531dd1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "def evaluate(dl,model_new=None,path=None,modelclass=None):\n",
    "    assert (model_new is not None) ^ (path is not None)\n",
    "    if path is not None:\n",
    "        model_new=modelclass().cuda()\n",
    "        model_new.load_state_dict(torch.load(path))\n",
    "    loader = tqdm(dl, total=len(dl), unit=\"batches\")\n",
    "    total_len=0\n",
    "    model_new.eval()    \n",
    "    with torch.no_grad():\n",
    "        total_loss=0\n",
    "        tts=0\n",
    "        y_pred=[]\n",
    "        y_true=[]\n",
    "        for batch in loader:\n",
    "            input_ids,attn_mask,y=batch\n",
    "            classfn_out,loss=model_new(input_ids,attn_mask,y,use_decoder=False,use_classfn=1)\n",
    "            if classfn_out.ndim==1:\n",
    "                predict=torch.zeros_like(y)\n",
    "                predict[classfn_out>0]=1\n",
    "            else:\n",
    "                predict=torch.argmax(classfn_out,dim=1)\n",
    "                \n",
    "            y_pred.append(predict.cpu().numpy())\n",
    "            y_true.append(y.cpu().numpy())\n",
    "            total_loss+=(len(input_ids)*loss[0].item())\n",
    "            total_len+=len(input_ids)\n",
    "        total_loss=total_loss/total_len\n",
    "        mac_prec,mac_recall,mac_f1_score,_=precision_recall_fscore_support(np.concatenate(y_true),np.concatenate(y_pred),labels=[0,1])\n",
    "        mic_prec,mic_recall,mic_f1_score,_=0,0,0,0\n",
    "\n",
    "    return total_loss,mac_prec,mac_recall,mac_f1_score,mic_prec,mic_recall,mic_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0ae2e4c-4e43-4a4c-9d25-f0e0f7254e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prototypes=20\n",
    "num_pos_protos=19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a1c3091-f37d-4562-8c03-ec77524e623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegProtoBartModel(torch.nn.Module):\n",
    "    def __init__(self,n_classes=2,bias=True,dropout=False,special_classfn=False,p=0.5,batchnormlp1=False):\n",
    "        super().__init__()\n",
    "        self.bart_model=BartForConditionalGeneration.from_pretrained('facebook/bart-large') \n",
    "        self.bart_out_dim=self.bart_model.config.d_model\n",
    "        self.one_by_sqrt_bartoutdim=1/torch.sqrt(torch.tensor(self.bart_out_dim).float())\n",
    "        self.max_position_embeddings=256\n",
    "        self.num_protos=num_prototypes\n",
    "        self.num_pos_protos=num_pos_protos\n",
    "        self.num_neg_protos=self.num_protos-self.num_pos_protos\n",
    "        self.pos_prototypes=torch.nn.Parameter(torch.rand(self.num_pos_protos,self.max_position_embeddings,self.bart_out_dim))\n",
    "        self.neg_prototypes=torch.nn.Parameter(torch.rand(self.num_neg_protos,self.max_position_embeddings,self.bart_out_dim))\n",
    "        self.classfn_model=torch.nn.Linear(self.num_protos,2,bias=bias)\n",
    "        self.loss_fn=torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        \n",
    "        self.do_dropout=dropout\n",
    "        self.special_classfn=special_classfn\n",
    "        \n",
    "        self.dropout=torch.nn.Dropout(p=p)\n",
    "        self.dobatchnorm=batchnormlp1\n",
    "        self.distance_grounder = torch.zeros(2, self.num_protos).cuda()\n",
    "        self.distance_grounder[0][:self.num_pos_protos] = 1e7\n",
    "        self.distance_grounder[1][self.num_pos_protos:] = 1e7\n",
    "\n",
    "    \n",
    "    def set_prototypes(self,do_random=False):\n",
    "        if do_random:\n",
    "            print(\"initializing prototypes with xavier init\")\n",
    "            torch.nn.init.xavier_normal_(self.pos_prototypes)\n",
    "            torch.nn.init.xavier_normal_(self.neg_prototypes)\n",
    "        else:\n",
    "            print(\"initializing prototypes with encoded outputs\")\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                self.pos_prototypes=torch.nn.Parameter(\n",
    "                    self.bart_model.base_model.encoder(input_ids_pos_rdm.cuda(),\n",
    "                                                       attn_mask_pos_rdm.cuda(),\n",
    "                                                       output_attentions=False,\n",
    "                                                       output_hidden_states=False).last_hidden_state)\n",
    "                self.neg_prototypes=torch.nn.Parameter(\n",
    "                    self.bart_model.base_model.encoder(input_ids_neg_rdm.cuda(),\n",
    "                                                       attn_mask_neg_rdm.cuda(),\n",
    "                                                       output_attentions=False,\n",
    "                                                       output_hidden_states=False).last_hidden_state)\n",
    "    \n",
    "    def set_shared_status(self,status=True):\n",
    "        print(\"ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\")\n",
    "        self.bart_model.model.shared.requires_grad_(status)\n",
    "\n",
    "    def set_encoder_status(self,status=True):\n",
    "        self.num_enc_layers=len(self.bart_model.base_model.encoder.layers)\n",
    "        for i in range(self.num_enc_layers):\n",
    "            self.bart_model.base_model.encoder.layers[i].requires_grad_(False)\n",
    "        self.bart_model.base_model.encoder.layers[self.num_enc_layers-1].requires_grad_(status)\n",
    "        return\n",
    "    def set_decoder_status(self,status=True):\n",
    "        self.num_dec_layers=len(self.bart_model.base_model.decoder.layers)\n",
    "        for i in range(self.num_dec_layers):\n",
    "            self.bart_model.base_model.decoder.layers[i].requires_grad_(False)\n",
    "        self.bart_model.base_model.decoder.layers[self.num_dec_layers-1].requires_grad_(status)\n",
    "        return\n",
    "    def set_classfn_status(self,status=True):\n",
    "        self.classfn_model.requires_grad_(status)\n",
    "\n",
    "    def set_protos_status(self,pos_or_neg=None,status=True):\n",
    "        if pos_or_neg==\"pos\" or pos_or_neg is None:\n",
    "            self.pos_prototypes.requires_grad=status       \n",
    "        if pos_or_neg==\"neg\" or pos_or_neg is None:\n",
    "            self.neg_prototypes.requires_grad=status       \n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attn_mask, y, use_decoder=1, use_classfn=0, use_rc=0, use_p1=0, use_p2=0,\n",
    "                use_p3=0, classfn_lamb=1.0, rc_loss_lamb=0.95, p1_lamb=0.93, p2_lamb=0.92, p3_lamb=1.0,\n",
    "                distmask_lp1=0,distmask_lp2=0,\n",
    "                pos_or_neg=None,random_mask_for_distanceMat=None):\n",
    "        \"\"\"\n",
    "            1. p3_loss is the prototype-distance-maximising loss. See the set of lines after the line \"if use_p3:\"\n",
    "            2. We also have flags distmask_lp1 and distmask_lp2 which uses \"masked\" distance matrix for calculating lp1 and lp2 loss.\n",
    "            3. the flag \"random_mask_for_distanceMat\" is an experimental part. It randomly masks (artificially inflates) \n",
    "            random places in the distance matrix so as to encourage more prototypes be \"discovered\" by the training \n",
    "            examples.\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.size(0)\n",
    "        if use_decoder:\n",
    "            labels = input_ids.cuda() + 0\n",
    "            labels[labels == self.bart_model.config.pad_token_id] = -100\n",
    "            bart_output = self.bart_model(input_ids.cuda(), attn_mask.cuda(), labels=labels,\n",
    "                                          output_attentions=False, output_hidden_states=False)\n",
    "            rc_loss, last_hidden_state = bart_output.loss, bart_output.encoder_last_hidden_state\n",
    "        else:\n",
    "            rc_loss = torch.tensor(0)\n",
    "            last_hidden_state = self.bart_model.base_model.encoder(input_ids.cuda(), attn_mask.cuda(),\n",
    "                                                                   output_attentions=False,\n",
    "                                                                   output_hidden_states=False).last_hidden_state\n",
    "        input_for_classfn, l_p1, l_p2, l_p3, l_p4, classfn_out, classfn_loss = (None, torch.tensor(0), torch.tensor(0),\n",
    "                                                                                torch.tensor(0), torch.tensor(0), None,\n",
    "                                                                                torch.tensor(0))\n",
    "        if use_classfn or use_p1 or use_p2 or use_p3:\n",
    "            all_protos = torch.cat((self.pos_prototypes, self.neg_prototypes), dim=0)\n",
    "            if use_classfn or use_p1 or use_p2:\n",
    "                if not self.dobatchnorm:\n",
    "                    input_for_classfn = self.one_by_sqrt_bartoutdim * torch.cdist(last_hidden_state.view(batch_size, -1),\n",
    "                                                                                  all_protos.view(self.num_protos, -1))\n",
    "                else:\n",
    "                    input_for_classfn = torch.cdist(last_hidden_state.view(batch_size, -1),\n",
    "                                                    all_protos.view(self.num_protos, -1))\n",
    "                    input_for_classfn= torch.nn.functional.instance_norm(\n",
    "                        input_for_classfn.view(batch_size,1,self.num_protos)).view(batch_size,\n",
    "                                                                                   self.num_protos)\n",
    "            if use_p1 or use_p2:\n",
    "                distance_mask = self.distance_grounder[y.cuda()]\n",
    "                input_for_classfn_masked = input_for_classfn+distance_mask\n",
    "                if random_mask_for_distanceMat:\n",
    "                    random_mask=torch.bernoulli(torch.ones_like(input_for_classfn_masked)*\n",
    "                                                random_mask_for_distanceMat).bool()\n",
    "                    input_for_classfn_masked[random_mask]=1e7\n",
    "        if use_p1:\n",
    "            l_p1 = torch.mean(torch.min(input_for_classfn_masked if distmask_lp1 else input_for_classfn, dim=0)[0])\n",
    "        if use_p2:\n",
    "            l_p2 = torch.mean(torch.min(input_for_classfn_masked if distmask_lp2 else input_for_classfn, dim=1)[0])\n",
    "        if use_p3:\n",
    "            l_p3 = self.one_by_sqrt_bartoutdim * torch.mean(torch.pdist(\n",
    "                self.pos_prototypes.view(self.num_pos_protos,-1)))\n",
    "        if use_classfn:\n",
    "            if self.do_dropout:\n",
    "                if self.special_classfn:\n",
    "                    classfn_out = (input_for_classfn@self.classfn_model.weight.t()+\n",
    "                                   self.dropout(self.classfn_model.bias.repeat(batch_size,1))).view(batch_size, 2)\n",
    "                else:\n",
    "                    classfn_out = self.classfn_model(self.dropout(input_for_classfn)).view(batch_size, 2)\n",
    "            else:\n",
    "                classfn_out = self.classfn_model(input_for_classfn).view(batch_size, 2)\n",
    "            classfn_loss = self.loss_fn(classfn_out, y.cuda())\n",
    "        if not use_rc:\n",
    "            rc_loss = torch.tensor(0)\n",
    "        total_loss = classfn_lamb * classfn_loss + rc_loss_lamb * rc_loss + p1_lamb * l_p1 + p2_lamb * l_p2 - p3_lamb * l_p3\n",
    "        return classfn_out, (total_loss, classfn_loss.detach().cpu(), rc_loss.detach().cpu(), l_p1.detach().cpu(),\n",
    "                             l_p2.detach().cpu(), l_p3.detach().cpu())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df9c9a4c-f945-4df5-8b05-7c75bf4c4942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing prototypes with xavier init\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()        \n",
    "# model=SimpleBartModel().cuda()\n",
    "# model=SimpleProtoBartModel().cuda()\n",
    "modelname=\"NegProtoTEx_protos_xavier_large_bs20_20_woRat_noReco_g2d_nobias_nodrop_cu1_PosUp_normed\"\n",
    "model=NegProtoBartModel(bias=False,dropout=False,special_classfn=False,p=0.75,batchnormlp1=True).cuda()\n",
    "model.set_prototypes(do_random=True)\n",
    "torch.cuda.empty_cache()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dc5bb39-c4ba-4a17-8599-08a1616d53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"../Models/\"+modelname\n",
    "logs_path=\"../Logs/\"+modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bb20877-aca8-4173-a1d9-78ea0a00bd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/05773/anubrata/ls6/anaconda3/envs/prototex/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8abe58f3dc409f802474ad1b3f4894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch -1 Total loss 0.7214 \n",
      "\n",
      "VAL SCORES epoch -1 Prec [0.68705548 0.34420015] \n",
      "\n",
      "VAL SCORES epoch -1 Recall [0.35830861 0.6735905 ] \n",
      "\n",
      "VAL SCORES epoch -1 F1 [0.47098976 0.45559458] \n",
      "\n",
      "\n",
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdac501e4fbf49b288c3ad1b7f4de43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "delta training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c1071bd7744672b5b86ffbb3f4a3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d3157509734705bf85bee41af85ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dba879a36b9436cb4f2604155b067ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 0 Total loss 0.5537 \n",
      "\n",
      "TRAIN SCORES epoch 0 Prec [0.76863928 0.77208202] \n",
      "\n",
      "TRAIN SCORES epoch 0 Recall [0.77353316 0.76716623] \n",
      "\n",
      "TRAIN SCORES epoch 0 F1 [0.77107846 0.76961627] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db38c679d1f246b48ebe9b34655e791a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 0 Total loss 0.5812 \n",
      "\n",
      "VAL SCORES epoch 0 Prec [0.86443662 0.58690745] \n",
      "\n",
      "VAL SCORES epoch 0 Recall [0.72848665 0.77151335] \n",
      "\n",
      "VAL SCORES epoch 0 F1 [0.79066023 0.66666667] \n",
      "\n",
      "\n",
      "\u001b[92m Validation score improved (-inf --> 0.7287). \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c34457d62044a8b9201e0510be87e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SCORES epoch 0 Total loss 0.5668 \n",
      "\n",
      "TEST SCORES epoch 0 Prec [0.89676665 0.55861582] \n",
      "\n",
      "TEST SCORES epoch 0 Recall [0.78647079 0.74905303] \n",
      "\n",
      "TEST SCORES epoch 0 F1 [0.8380051  0.63996764] \n",
      "\n",
      "\n",
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31de89f6f2ce429e8c0cace4f2f03fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "delta training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05967fd9ecef47ddb58d2299d76e8e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1499833943b452fbcd70c7268a8ee22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a37e08e9fe4dfb9044f51e9e7d9b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 1 Total loss 0.4821 \n",
      "\n",
      "TRAIN SCORES epoch 1 Prec [0.78876886 0.85092763] \n",
      "\n",
      "TRAIN SCORES epoch 1 Recall [0.86541287 0.76824371] \n",
      "\n",
      "TRAIN SCORES epoch 1 F1 [0.82531527 0.80747452] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1527f7f23e4237996c307e56522ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 1 Total loss 0.5213 \n",
      "\n",
      "VAL SCORES epoch 1 Prec [0.83281734 0.62739726] \n",
      "\n",
      "VAL SCORES epoch 1 Recall [0.79821958 0.67952522] \n",
      "\n",
      "VAL SCORES epoch 1 F1 [0.81515152 0.65242165] \n",
      "\n",
      "\n",
      "\u001b[92m Validation score improved (0.7287 --> 0.7338). \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2bd11bcb424f45b8f04cd40452437f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SCORES epoch 1 Total loss 0.4907 \n",
      "\n",
      "TEST SCORES epoch 1 Prec [0.88315018 0.58818835] \n",
      "\n",
      "TEST SCORES epoch 1 Recall [0.82371028 0.69791667] \n",
      "\n",
      "TEST SCORES epoch 1 F1 [0.85239526 0.63837159] \n",
      "\n",
      "\n",
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631459075dd94a11a4c42433d5dca293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "delta training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045f9e47f8dc49c68485cc96fcbe80a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3d5daaf7ff4a41afec32f30e794c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335e8739f2ac4a80bf807919c337cc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 2 Total loss 0.3835 \n",
      "\n",
      "TRAIN SCORES epoch 2 Prec [0.85330914 0.87192118] \n",
      "\n",
      "TRAIN SCORES epoch 2 Recall [0.87520815 0.84954452] \n",
      "\n",
      "TRAIN SCORES epoch 2 F1 [0.86411992 0.86058742] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c60612949d942948ae21cf82eff18cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 2 Total loss 0.5132 \n",
      "\n",
      "VAL SCORES epoch 2 Prec [0.83466454 0.60649351] \n",
      "\n",
      "VAL SCORES epoch 2 Recall [0.77522255 0.69287834] \n",
      "\n",
      "VAL SCORES epoch 2 F1 [0.80384615 0.6468144 ] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 1 out of 7 \u001b[0m\n",
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886f46dd25e1420488f1396ac4fd57e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "delta training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1a2b68993f46248afbb2fa6a241ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892ad75518ec474eb379d0fc83c12bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c15a35974f54c98b04ec48069b028d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 3 Total loss 0.2992 \n",
      "\n",
      "TRAIN SCORES epoch 3 Prec [0.89879779 0.89406037] \n",
      "\n",
      "TRAIN SCORES epoch 3 Recall [0.89342737 0.89940249] \n",
      "\n",
      "TRAIN SCORES epoch 3 F1 [0.89610453 0.89672347] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b428e0d7d448f6a647bf6ed1b3373f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 3 Total loss 0.5356 \n",
      "\n",
      "VAL SCORES epoch 3 Prec [0.82725832 0.6       ] \n",
      "\n",
      "VAL SCORES epoch 3 Recall [0.77448071 0.67655786] \n",
      "\n",
      "VAL SCORES epoch 3 F1 [0.8        0.63598326] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 2 out of 7 \u001b[0m\n",
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee101b76d173473198651f79b20b62e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "delta training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cd257a99ed4edfbf7bf1596debd5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61eacc56b6ba40cd8fc6c6824b0fc290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76987ccafca4228b3625c739e872f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 4 Total loss 0.2798 \n",
      "\n",
      "TRAIN SCORES epoch 4 Prec [0.90499953 0.93906298] \n",
      "\n",
      "TRAIN SCORES epoch 4 Recall [0.94152219 0.90116564] \n",
      "\n",
      "TRAIN SCORES epoch 4 F1 [0.92289966 0.91972408] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd9fdd602344108916d425328a163eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 4 Total loss 0.5133 \n",
      "\n",
      "VAL SCORES epoch 4 Prec [0.81511372 0.64036419] \n",
      "\n",
      "VAL SCORES epoch 4 Recall [0.82418398 0.62611276] \n",
      "\n",
      "VAL SCORES epoch 4 F1 [0.81962376 0.63315829] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 3 out of 7 \u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06302b468144e019d87212181702275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SCORES (not the best ones) epoch 4 Total loss 0.4466 \n",
      "\n",
      "TEST SCORES (not the best ones) epoch 4 Prec [0.87156283 0.61891892] \n",
      "\n",
      "TEST SCORES (not the best ones) epoch 4 Recall [0.85548343 0.65056818] \n",
      "\n",
      "TEST SCORES (not the best ones) epoch 4 F1 [0.86344828 0.63434903] \n",
      "\n",
      "\n",
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200def00eef346948b2f7f2bed835606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "delta training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413027855b6746a78555fe10a125c10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f6f5e343424d6191fb7afbf90cd3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f39e1533364b7ba75946bf44b1a3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 5 Total loss 0.2084 \n",
      "\n",
      "TRAIN SCORES epoch 5 Prec [0.95835866 0.92889647] \n",
      "\n",
      "TRAIN SCORES epoch 5 Recall [0.92653541 0.9597414 ] \n",
      "\n",
      "TRAIN SCORES epoch 5 F1 [0.9421784  0.94406706] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b44c930897e4c0e8c955eb112f7e44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 5 Total loss 0.5830 \n",
      "\n",
      "VAL SCORES epoch 5 Prec [0.82674051 0.60026385] \n",
      "\n",
      "VAL SCORES epoch 5 Recall [0.77522255 0.67507418] \n",
      "\n",
      "VAL SCORES epoch 5 F1 [0.80015314 0.63547486] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 4 out of 7 \u001b[0m\n",
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91922ceb71947b4863e728f5e1541e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "delta training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a470cb722194634860109ee53872ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4db1bf311d49c08c6d873ac2502ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28aa853c8b5429ab0b71cea019b6ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 6 Total loss 0.1656 \n",
      "\n",
      "TRAIN SCORES epoch 6 Prec [0.97082954 0.94082504] \n",
      "\n",
      "TRAIN SCORES epoch 6 Recall [0.93887746 0.9717896 ] \n",
      "\n",
      "TRAIN SCORES epoch 6 F1 [0.9545862  0.95605666] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa249402a32041a5bdff5c0f84a4df5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 6 Total loss 0.6779 \n",
      "\n",
      "VAL SCORES epoch 6 Prec [0.82380952 0.59317585] \n",
      "\n",
      "VAL SCORES epoch 6 Recall [0.77002967 0.67062315] \n",
      "\n",
      "VAL SCORES epoch 6 F1 [0.79601227 0.62952646] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 5 out of 7 \u001b[0m\n",
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2eb891542449dfba2461da57f165f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "delta training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2313a0d25fa4a0399a98a4a3043ce42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9a45273c3b4174a70b12ad656ac824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5d41e720144a6e86ca89f2f57b9a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 7 Total loss 0.1660 \n",
      "\n",
      "TRAIN SCORES epoch 7 Prec [0.98057651 0.93314089] \n",
      "\n",
      "TRAIN SCORES epoch 7 Recall [0.9296699  0.98158488] \n",
      "\n",
      "TRAIN SCORES epoch 7 F1 [0.95444489 0.95675005] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c3b45a3e24490fa62471d004680334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 7 Total loss 0.7088 \n",
      "\n",
      "VAL SCORES epoch 7 Prec [0.83       0.57177616] \n",
      "\n",
      "VAL SCORES epoch 7 Recall [0.7388724  0.69732938] \n",
      "\n",
      "VAL SCORES epoch 7 F1 [0.78178964 0.62834225] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 6 out of 7 \u001b[0m\n",
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1cfbc34f004bb29a806bcebc0defad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "delta training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT!!! Shared variable is shared by encoder_input_embeddings and decoder_input_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062c7ae0ac8b4f828a4e22c3f3b10a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a17997422614936884ad2d590a4636f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gamma training:   0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd87d5c5c2994fefa34a6ffaa0823c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1021 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORES epoch 8 Total loss 0.1776 \n",
      "\n",
      "TRAIN SCORES epoch 8 Prec [0.9876881  0.92459918] \n",
      "\n",
      "TRAIN SCORES epoch 8 Recall [0.91938486 0.98853952] \n",
      "\n",
      "TRAIN SCORES epoch 8 F1 [0.95231331 0.95550085] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c429fe3a5284b83bb7cab29d0deab0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL SCORES epoch 8 Total loss 0.7808 \n",
      "\n",
      "VAL SCORES epoch 8 Prec [0.83423423 0.5372807 ] \n",
      "\n",
      "VAL SCORES epoch 8 Recall [0.68694362 0.72700297] \n",
      "\n",
      "VAL SCORES epoch 8 F1 [0.7534581  0.61790668] \n",
      "\n",
      "\n",
      "\u001b[93m EarlyStopping counter: 7 out of 7 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "negative protobart\n",
    "\"\"\"\n",
    "from transformers.optimization import AdamW\n",
    "optim=AdamW(model.parameters(),lr=3e-5,weight_decay=0.01,eps=1e-8)\n",
    "f=open(logs_path,\"w\")\n",
    "f.writelines([\"\"])\n",
    "f.close()\n",
    "val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(val_dl,model)\n",
    "epoch=-1\n",
    "print_logs(logs_path,\"VAL SCORES\",epoch,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "es=EarlyStopping(-np.inf,patience=7,path=save_path,save_epochwise=False)\n",
    "n_iters=1000\n",
    "gamma=2\n",
    "delta=1\n",
    "kappa=1\n",
    "p1_lamb=0.9\n",
    "p2_lamb=0.9\n",
    "p3_lamb=0.9\n",
    "for iter_ in range(n_iters):\n",
    "    total_loss = 0\n",
    "    \"\"\"\n",
    "    During Delta, We want decoder to become better at decoding the trained encoder\n",
    "    and Prototypes to become closer to some encoded representation. And that's why it makes \n",
    "    sense to use l_p1 loss and not l_p2 loss.\n",
    "    losses- rc_loss, l_p1 loss\n",
    "    trainable- decoder and prototypes\n",
    "    details- makes pos_prototypes closer to pos_egs and neg_protos closer to neg_egs \n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model.set_encoder_status(status=False)\n",
    "    model.set_decoder_status(status=False)\n",
    "    model.set_protos_status(status=True)\n",
    "    model.set_classfn_status(status=False)\n",
    "    model.set_shared_status(status=True)\n",
    "\n",
    "    for epoch in range(delta):\n",
    "        train_loader = tqdm(train_dl, total=len(train_dl), unit=\"batches\", desc=\"delta training\")\n",
    "        for batch in train_loader:\n",
    "            input_ids, attn_mask, y = batch\n",
    "            classfn_out, loss = model(input_ids, attn_mask, y, use_decoder=0, use_classfn=0,\n",
    "                                      use_rc=0, use_p1=1, use_p2=0, use_p3=0,\n",
    "                                      rc_loss_lamb=1.0, p1_lamb=p1_lamb, p2_lamb=p2_lamb,\n",
    "                                      p3_lamb=p3_lamb,distmask_lp1=1,distmask_lp2=1,\n",
    "                                      random_mask_for_distanceMat=None)\n",
    "            optim.zero_grad()\n",
    "            loss[0].backward()\n",
    "            optim.step()\n",
    "    \"\"\"\n",
    "    During gamma, we only want to improve the classification performance. Therefore we will\n",
    "    improve encoder to become closer to the prototypes, at the same time also improving\n",
    "    the classification accuracy. That's why encoder and classification layer must be trainabl\n",
    "    together without segrregating pos and neg examples.\n",
    "    Only Encoder and Classfn are trainable\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model.set_encoder_status(status=True)\n",
    "    model.set_decoder_status(status=False)\n",
    "    model.set_protos_status(status=False)\n",
    "    model.set_classfn_status(status=True)\n",
    "    model.set_shared_status(status=True)\n",
    "\n",
    "    for epoch in range(gamma):\n",
    "        train_loader = tqdm(train_dl, total=len(train_dl), unit=\"batches\", desc=\"gamma training\")\n",
    "        for batch in train_loader:\n",
    "            input_ids, attn_mask, y = batch\n",
    "            classfn_out, loss = model(input_ids, attn_mask, y, use_decoder=0, use_classfn=1,\n",
    "                                      use_rc=0, use_p1=0, use_p2=1,\n",
    "                                      rc_loss_lamb=1., p1_lamb=p1_lamb,p2_lamb=p2_lamb,\n",
    "                                      distmask_lp1 = 1, distmask_lp2 = 1)\n",
    "            optim.zero_grad()\n",
    "            loss[0].backward()\n",
    "            optim.step()\n",
    "\n",
    "    val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(train_dl,model)\n",
    "    print_logs(logs_path,\"TRAIN SCORES\",iter_,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "    es.activate(mac_val_f1[0],mac_val_f1[1])\n",
    "\n",
    "    val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(val_dl,model)\n",
    "    print_logs(logs_path,\"VAL SCORES\",iter_,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)        \n",
    "    es(0.5*(mac_val_f1[1]+mac_val_f1[0]),epoch,model)\n",
    "    if es.early_stop:\n",
    "        break\n",
    "    if es.improved:\n",
    "        \"\"\"\n",
    "        Below using \"val_\" prefix but the dl is that of test.\n",
    "        \"\"\"\n",
    "        val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(test_dl,model)\n",
    "        print_logs(logs_path,\"TEST SCORES\",iter_,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)\n",
    "    elif (iter_+1)%5==0:\n",
    "        \"\"\"\n",
    "        Below using \"val_\" prefix but the dl is that of test.\n",
    "        \"\"\"\n",
    "        val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1=evaluate(test_dl,model)\n",
    "        print_logs(logs_path,\"TEST SCORES (not the best ones)\",iter_,val_loss,mac_val_prec,mac_val_rec,mac_val_f1,mic_val_prec,mic_val_rec,mic_val_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prototex",
   "language": "python",
   "name": "prototex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
